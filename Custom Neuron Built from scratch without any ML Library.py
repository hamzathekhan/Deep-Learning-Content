# -*- coding: utf-8 -*-
"""Deep Learning Projects.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FnvNo8vhF_aEAM9Psl7bbKIMsnO89nV-
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
# %matplotlib inline

from random import randint
data = {
    "age" : [randint(10,90) for i in range(50)],
    "afford" : [randint(0,1) for i in range(50)],
    "insurance" : [randint(0,1) for i in range(50)],
}

df = pd.DataFrame(data)
df

from sklearn.model_selection import train_test_split

x_train , x_test , y_train , y_test = train_test_split(df[["age","afford"]],df["insurance"],test_size=0.2,
                                                       random_state=42)

x_train_scaled = x_train.copy()
x_train_scaled["age"] = x_train_scaled["age"]/100

x_test_scaled = x_test.copy()
x_test_scaled["age"] = x_test_scaled["age"]/100

model = keras.Sequential([
    keras.layers.Dense(1,input_shape=(2,),activation="sigmoid",kernel_initializer="ones",bias_initializer="zeros")
])

model.compile(optimizer="adam",loss="binary_crossentropy",metrics=["accuracy"])

model.fit(x_train,y_train,epochs=300)

model.evaluate(x_test,y_test)

y_predicted = model.predict(x_test)
y_predicted = y_predicted.flatten()

coef  , intercept = model.get_weights()

coef , intercept

def sigmoid(x):
  return 1/(1+np.exp(-x))

def log_loss(y_true,y_predicted):
  epsilon = 1e-15
  y_predicted_new = [max(i,epsilon) for i in y_predicted]
  y_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]
  y_predicted_new = np.array(y_predicted_new)
  return -np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))

log_loss(y_test,y_predicted)

class myNN():
  def __init__(self):
    self.w1 = 1
    self.w2 = 2
    self.bias = 0

  def fit(self,x,y,epochs,loss_threshold):
    self.w1 , self.w2 , self.bias = self.gradient_descent(x["age"],x["afford"],y,epochs,loss_threshold)

  def predict(self,x_test):
    w_sum = self.w1 * x_test["age"] + self.w2 * x_test["afford"] + self.bias
    return sigmoid(w_sum)

  def gradient_descent(self,age,afford,y_true,epochs,loss_threshold):
    age = np.array(age)
    afford = np.array(afford)
    y_true = np.array(y_true)

    w1 = w2 = 1
    bias = 0
    rate = 0.5
    n = len(age)

    for i in range(epochs):
      w_sum = w1 * age + w2 * afford + bias
      y_predicted = sigmoid(w_sum)

      loss = log_loss(y_true,y_predicted)

      w1d = (1/n)*np.dot(np.transpose(age),(y_predicted-y_true))
      w2d = (1/n)*np.dot(np.transpose(afford),(y_predicted-y_true))

      bias_d = np.mean(y_predicted-y_true)

      w1 = w1 - rate * w1d
      w2 = w2 - rate * w2d
      bias = bias - rate * bias_d

      if i%50 == 0:
        print(f"Epoch:{i}, w1:{w1}, w2:{w2}, bias:{bias}, loss:{loss}")

      if loss <= loss_threshold :
        break

    return w1,w2,bias

cusmodel = myNN()
cusmodel.fit(x_train_scaled,y_train,epochs=1000,loss_threshold=0)
cusmodel.predict(x_test_scaled)

model.predict(x_test_scaled)

coef , intercept