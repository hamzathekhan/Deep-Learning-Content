# -*- coding: utf-8 -*-
"""Deep Learning Projects.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FnvNo8vhF_aEAM9Psl7bbKIMsnO89nV-
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
# %matplotlib inline

from random import randint
data = {
    "age" : [randint(10,90) for i in range(50)],
    "afford" : [randint(0,1) for i in range(50)],
    "insurance" : [randint(0,1) for i in range(50)],
}

df = pd.DataFrame(data)
df.head(5)

from sklearn.model_selection import train_test_split

x_train , x_test , y_train , y_test = train_test_split(df[["age","afford"]],df["insurance"],test_size=0.2)

x_train_scaled = x_train.copy()
x_train_scaled["age"] = x_train_scaled["age"]/100

x_test_scaled = x_test.copy()
x_test_scaled["age"] = x_test_scaled["age"]/100

class customNN():
  def __init__(self):
    self.w1 = 1
    self.w2 = 1
    self.bias = 0


  def sigmoid(self,x):
    return 1/(1+np.exp(-x))

  def log_loss(self,y_true,y_predicted):
    epsilon = 1e-15
    y_predicted_new = [max(i,epsilon) for i in y_predicted]
    y_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]
    y_predicted_new = np.array(y_predicted_new)
    return -(np.mean(y_true * np.log(y_predicted_new)+(1-y_true)* np.log(1-y_predicted_new)))


  def gradient_descent(self,age,afford,y_true,epochs,loss_threshold=0):
    w1 = w2 = 1
    bias = 0
    rate = 0.5
    n = len(age)

    for i in range(epochs):
      w_sum = w1 * age + w2 * afford + bias
      y_predicted = self.sigmoid(w_sum)

      loss = self.log_loss(y_true,y_predicted)

      w1d = (1/n)*np.dot(np.transpose(age),(y_predicted-y_true))
      w2d = (1/n)*np.dot(np.transpose(afford),(y_predicted-y_true))

      bias_d = np.mean(y_predicted-y_true)

      w1 = w1 - rate * w1d
      w2 = w2 - rate * w2d
      bias = bias - rate * bias_d

      if i+1 == epochs:
        print(f"Epoch:{i}, w1:{w1}, w2:{w2}, bias:{bias}, loss:{loss}")
    return w1 , w2 , bias

  def fit(self,x,y,epochs,loss_threshold=0):
    self.w1 , self.w2 , self.bias = self.gradient_descent(x["age"],x["afford"],y,epochs,loss_threshold)

  def predict(self,x_test):
    w_sum = self.w1 * x_test["age"] + self.w2 * x_test["afford"] + self.bias
    return self.sigmoid(w_sum)

custom_model = customNN()
custom_model.fit(x_train_scaled,y_train,5)

custom_model.predict(x_test_scaled)

y_test